

# 1. Use a better algorithm.

! The selection sort algorithm above is faster than the bubble sort for random data, but the bubble sort is much faster for sorted data. ***The relationship between inputs and algorithmic performance can be subtle***. Famously, if you choose an unfortunate “pivot” when implementing [quicksort](https://en.wikipedia.org/wiki/Quicksort), you’ll find that it is very non-quick (e.g. you can make it as slow on already-sorted data as the selection sort above).

=> “use a better algorithm” requires understanding the wider context of your system and the nature of the algorithm you’re thinking of using.

# 2. Use a better data-structure

Just as with “use a better algorithm”, “use a better data-structure” requires careful thought and measurement.

! There is an important tactical variant on “better data-structures” that is perhaps best thought of as ***“put your structs/classes on a diet”***.
	1. reducing objects size
	2. reducing “pointer chasing” (typically by folding multiple structs/classes into one)
	3. encouraging memory locality <= it’s difficult to measure the indirect impact

# 3. Use a lower-level system

1. check that you’ve got compiler optimisations turned on, 
2. use faster libraries or databases

Sometimes rewriting in a lower-level programming language really is the right thing to do, but it is rarely a quick job, and it inevitably ***introduces a period of instability while bugs are shaken out of the new version.***

# 4. Accept a less precise solution

[local search](https://en.wikipedia.org/wiki/Local_search_(optimization)). => we define a metric (in our running example, how fast our benchmark suite runs) that allows us to compare two solutions (in our case, faster is better) and discard the worst. 
	1. [fast inverse square root](https://en.wikipedia.org/wiki/Fast_inverse_square_root) approximates multiplicative inverse
	2. [Bloom filter](https://en.wikipedia.org/wiki/Bloom_filter) can give false positives: accepting that possibility allows it to be exceptionally frugal with memory
	3. JPEG image compression deliberately throws away some of an image’s fine details in order to make the image more compressible

***Unless you are convinced you can tolerate incorrectness, you’re best off assuming that you can’t.***

# 5. Use [cache](0.%20Cache.md)

Without caching, the server performs redundant computations for similar requests, leading to slower response times. It regenerates data for each request, straining resources like CPU and memory.

# 6. Check Network

 The physical distance between client and server introduces latency due to the finite speed of light, causing delays in data transmission. **Network congestion** from routers and switches can lead to packet delays or loss, exacerbating latency. **Limited network bandwidth** can restrict data transmission, resulting in queuing delays, especially during peak usage periods. Network protocols add processing overhead, contributing to latency. In wireless networks, factors like signal interference and environmental conditions can further increase latency compared to wired connections.
 
 **Network round trips** can add significant latency to API requests. To minimize network round trips, it is important to use techniques such as 
	 **batch processing** (**request coalescing**) 
	 **asynchronous processing**, 
	 **compressed data transfer** (*GZIP or Brotli can be used to compress data efficiently, providing a good balance between compressed size and decompression speed*)

! **Larger payloads** also increase network latency, especially over networks with limited bandwidth or high latency, causing delays in delivering API responses

# 7.  Efficient Data Handling

1. **Normalize data** to minimize redundancy and ensure data integrity, or denormalize data for improved performance by reducing joins.
2. Use **lazy loading** to fetch related data only when needed, or employ **eager loading** to minimize subsequent queries. Perform batch operations whenever possible to reduce database round trips and improve efficiency.
3. **Parallel call** : simultaneously calling multiple remote APIs to fetch data concurrently, significantly reducing the overall time taken instead of using **Series call** — Sequentially calling multiple remote APIs one after the other to gather necessary data. This leads to poor performance as the total time taken is the sum of the taken by each call.
4. **Use Efficient Data Transfer Formats**: Choose lightweight and efficient formats like JSON or Protocol Buffers to minimize data size and improve response times by less bandwidth and time for transmission

# 8. Large [Transactions](1.%20Software%20Engineering/3.%20Database/OTLP/SQL/3.%20Transactions/_Base.md)

Problems with long-running transactions:
- **Deadlock** — Transactions waiting on each other can cause a system halt.
- **Long Rollback Times** — Long transactions can take time to roll back, affecting performance.
- **Connection Pool Exhaustion in High Concurrency** — High concurrency can lead to the exhaustion of database connections.
- **Lock Waits** — Transactions waiting for locks can delay operations.
- **Timeouts** — Long transactions may exceed timeout limits, causing failures.
- **Database Master-Slave Delay** — Replication delays between master and slave databases can lead to stale reads.

=>
1. Execute non-transactional SELECT queries outside the transaction boundary to avoid unnecessary rollback overhead
2. Minimise remote calls within transactions to prevent latency. If unavoidable, ensure efficiency and implement timeouts
3. Process data in smaller batches to reduce resource contention
4. Execute operations not requiring transactional consistency outside transactions, like read-only tasks
5. Consider asynchronous processing for independent tasks

# 9. **Lock Granularity**

In business scenarios, locking mechanisms prevent multiple threads from concurrently modifying shared data, avoiding data anomalies. However, improper locking can lead to coarse lock granularity, impacting performance.

1. **Method-Level Locking** Locking an entire method can degrade performance if only part of the method requires locking. => to improve performance, lock only the critical section of the code
2. **Database Distributed [Lock](Transaction%20Range%20Locks.md)**: the optimisation direction for database locks = Row locks > Gap Locks > Table Locks

# 10. **Employ Asynchronous Operations**

Use asynchronous processing for long-running or computationally intensive tasks. By offloading such operations to background processes or queues, the API can remain responsive, preventing delays and improving overall efficiency

> By executing long-running tasks asynchronously, APIs can avoid blocking other requests and provide timely responses to clients

(+) enables faster response times and enhances the overall responsiveness of the API
(+) allows APIs to efficiently handle high volumes of concurrent requests by not tying up resources while waiting for completion
(+) helps optimize resource utilization and improve overall system performance
(+) enhance the fault tolerance of APIs: if a downstream service is temporarily unavailable, asynchronous processing allows the API to continue processing other requests and handle the error condition asynchronously.

# 11. [Rate Limiting](1.%20Software%20Engineering/2.%20Architecture/1.%20Concepts/Rate%20Limiting.md) and [[Throttling]]

**Rate Limiting**: prevent API abuse, protect server resources, and maintain a consistent quality of service.

**Throttling** is useful for managing system load and preventing overwhelming spikes in traffic that can lead to performance degradation or service disruptions

**How to?**
1. Prioritize user experience (limits aren't overly restrictive or burdensome for legitimate users)
2. Evaluate the capacity of your API resources to determine appropriate limits that maintain optimal performance without exhausting resources
3. Align rate limits with business requirements, taking into account different service tiers or levels
4. Consider bursts of requests during peak periods and implement suitable limits to handle them

**Steps:**
1. **Set Reasonable Limits**: balance between meeting user needs and protecting your API resources from abuse or overload
2. **Use Quotas and Time Windows**: allowing a certain number of requests per minute or per hour, to distribute API usage fairly. Consider using sliding time windows to prevent bursts of requests from exceeding the limits.
3. **Implement Token-Based Systems**: require clients to authenticate and obtain tokens or API keys.
4. **Provide Granular Rate Limiting**: rate limiting at various levels, such as per user, per-IP address, per-API key, or per endpoint. Some endpoints may be more resource-intensive and require stricter limits, while others may have more relaxed limits.
5. **Graceful Error Handling**: when rate limits are exceeded, provide clear and informative error responses to clients.
6. **Monitor and Analyze Usage Patterns**: insights into traffic patterns and detect any anomalies or potential security threats.

# References:

1. ~~[Four Kinds of Optimisation](https://tratt.net/laurie/blog/2023/four_kinds_of_optimisation.html)~~
2. ~~[Mastering API Performance Optimization Part 1](https://jinlow.medium.com/mastering-api-performance-optimization-1-4f82ac5cf792)~~
3. ~~[Mastering API Performance Optimization Part 2](https://jinlow.medium.com/mastering-api-performance-optimization-2-2b078f893272)~~
4. ~~[API Design Principles for Optimal Performance and Scalability](https://dzone.com/articles/api-design-principles-for-optimal-performance-and-2)~~
5. ~~[Designing High-Performance APIs](https://dzone.com/articles/designing-high-performance-apis)~~